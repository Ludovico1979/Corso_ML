{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearning-NChain-OpenGym.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thimotyb/real-world-machine-learning/blob/python3/ReinforcementLearning_NChain_OpenGym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwLWlFPuPLoB",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning\n",
        "## Open-AI Gym n-chain example with neural networks\n",
        "[link text](https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddtIhVpIPT3c",
        "colab_type": "text"
      },
      "source": [
        "The NChain example on Open AI Gym is a simple 5 state environment. There are two\n",
        "possible actions in each state, move forward (action 0) and move backwards (action 1).\n",
        "When action 1 is taken, i.e. move backwards, there is an immediate reward of 2 given to\n",
        "the agent – and the agent is returned to state 0 (back to the beginning of the chain).\n",
        "However, when a move forward action is taken (action 0), there is no immediate reward\n",
        "until state 4. When the agent moves forward while in state 4, a reward of 10 is received by\n",
        "the agent. The agent stays in state 4 at this point also, so the reward can be repeated.\n",
        "There is also a random chance that the agent’s action is “flipped” by the environment (i.e.\n",
        "an action 0 is flipped to an action 1 and vice versa). The diagram below demonstrates this\n",
        "environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWmjxGnFPaIO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzlo_aGpQBUq",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning – the basics\n",
        "Reinforcement learning can be considered the third genre of the machine learning triad – unsupervised learning, supervised learning and reinforcement learning. In supervised learning, we supply the machine learning system with curated (x, y) training pairs, where the intention is for the network to learn to map x to y. In reinforcement learning, we create an agent which performs actions in an environment and the agent receives various rewards depending on what state it is in when it performs the action. In other words, an agent explores a kind of game, and it is trained by trying to maximize rewards in this game. This cycle is illustrated in the figure below:\n",
        "\n",
        "![alt text](https://i2.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png?w=381&ssl=1)\n",
        "\n",
        "As can be observed above, the agent performs some action in the environment. An interpreter views this action in the environment, and feeds back an updated state that the agent now resides in, and also the reward for taking this action. The environment is not known by the agent beforehand, but rather it is discovered by the agent taking incremental steps in time. So, for instance, at time t the agent, in state ,  may take action a. This results in a new state  and a reward r. This reward can be a positive real number, zero, or a negative real number. It is the goal of the agent to learn which state dependent action to take which maximizes its rewards. The way which the agent optimally learns is the subject of reinforcement learning theory and methodologies.\n",
        "\n",
        "To more meaningfully examine the theory and possible approaches behind reinforcement learning, it is useful to have a simple example in which to work through. This simple example will come from an environment available on Open AI Gym called NChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZcAMmvQpNk",
        "colab_type": "text"
      },
      "source": [
        "## Playing with Gym\n",
        "\n",
        "The step() command returns 4 variables in a tuple, these are (in order):\n",
        "\n",
        "The new state after the action\n",
        "The reward due to the action\n",
        "Whether the game is “done” or not – the NChain game is done after 1,000 steps\n",
        "Debugging information – not relevant in this example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuscHiZjQteD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7141ca4-7da8-476a-98c0-c262cc2db8d9"
      },
      "source": [
        "# Prepare the Gym\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('NChain-v0')\n",
        "env.reset()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3T3lFvXQ1m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7805c3f3-b44c-4742-9e12-1f8cd5ea08c6"
      },
      "source": [
        "# Simulate a few steps of the game\n",
        "print(env.step(1))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0)) # Reaches the max reward in the game\n",
        "print(env.step(0))\n",
        "print(env.step(0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 2, False, {})\n",
            "(1, 0, False, {})\n",
            "(2, 0, False, {})\n",
            "(3, 0, False, {})\n",
            "(4, 0, False, {})\n",
            "(0, 2, False, {})\n",
            "(1, 0, False, {})\n",
            "(2, 0, False, {})\n",
            "(3, 0, False, {})\n",
            "(4, 0, False, {})\n",
            "(4, 10, False, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDdaZttRyKF",
        "colab_type": "text"
      },
      "source": [
        "As can be observed, starting in state 0 and taking step(1) action, the agent stays in state 0 and gets 2 for its reward. Next, I sent a series of action 0 commands. After every action 0 command, we would expect the progression of the agent along the chain, with the state increasing in increments (i.e. 0 -> 1 -> 2 etc.). However, you’ll observe after the first step(0) command, that the agent stays in state 0 and gets a 2 reward. This is because of the random tendency of the environment to “flip” the action occasionally, so the agent actually performed a 1 action. This is just unlucky.\n",
        "\n",
        "Nevertheless, I persevere and it can be observed that the state increments as expected, but there is no immediate reward for doing so for the agent until it reaches state 4. When in state 4, an action of 0 will keep the agent in step 4 and give the agent a 10 reward. Not only that, the environment allows this to be done repeatedly, as long as it doesn’t produce an unlucky “flip”, which would send the agent back to state 0 – the beginning of the chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcvF-cyTKnX",
        "colab_type": "text"
      },
      "source": [
        "# A first naive heuristic for reinforcement learning\n",
        "\n",
        "In order to train the agent effectively, we need to find a good policy pi which maps states to actions in an optimal way to maximize reward. There are various ways of going about finding a good or optimal policy, but first, let’s consider a naive approach.\n",
        "\n",
        "Let’s conceptualize a table, and call it a reward table, which looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAqznm8SNyl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "$\\begin{bmatrix}r_{s_0,a_0} & r_{s_0,a_1}\\\\ r_{s_1,a_0} & r_{s_1,a_1} \\\\\n",
        "r_{s_2,a_0} & r_{s_2,a_1} \\\\\n",
        "r_{s_3,a_0} & r_{s_3,a_1} \\\\\n",
        "r_{s_4,a_0} & r_{s_4,a_1} \\end{bmatrix}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VmbEHQATcQl",
        "colab_type": "text"
      },
      "source": [
        "Each of the rows corresponds to the 5 available states in the NChain environment, and each column corresponds to the 2 available actions in each state – forward and backward, 0 and 1. The value in each of these table cells corresponds to some measure of reward that the agent has “learnt” occurs when they are in that state and perform that action. So, the value  would be, say, the sum of the rewards that the agent has received when in the past they have been in state 0 and taken action 0. This table would then let the agent choose between actions based on the summated (or average, median etc. – take your pick) amount of reward the agent has received in the past when taking actions 0 or 1.\n",
        "\n",
        "This might be a good policy – choose the action resulting in the greatest previous summated reward. Let’s give it a try, the code looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ywx61KSLJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_sum_reward_agent(env, num_episodes=500):\n",
        "    # this is the table that will hold our summated rewards for\n",
        "    # each action in each state\n",
        "    r_table = np.zeros((5, 2))\n",
        "    for g in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.sum(r_table[s, :]) == 0:\n",
        "                # make a random selection of actions\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # select the action with highest cummulative reward\n",
        "                a = np.argmax(r_table[s, :])\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            r_table[s, a] += r\n",
        "            s = new_s\n",
        "    return r_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCXioZdZUEpn",
        "colab_type": "text"
      },
      "source": [
        "In the function definition, the environment is passed as the first argument, then the number of episodes (or number of games) that we will train the r_table on. We first create the r_table matrix which I presented previously and which will hold our summated rewards for each state and action. Then there is an outer loop which cycles through the number of episodes. The env.reset() command starts the game afresh each time a new episode is commenced. It also returns the starting state of the game, which is stored in the variable s.\n",
        "\n",
        "The second, inner loop continues until a “done” signal is returned after an action is passed to the environment. The if statement on the first line of the inner loop checks to see if there are any existing values in the r_table for the current state – it does this by confirming if the sum across the row is equal to 0. If it is zero, then an action is chosen at random – there is no better information available at this stage to judge which action to take.\n",
        "\n",
        "This condition will only last for a short period of time. After this point, there will be a value stored in at least one of the actions for each state, and the action will be chosen based on which column value is the largest for the row state s. In the code, this choice of the maximum column is executed by the numpy argmax function – this function returns the index of the vector / matrix with the highest value. For example, if the agent is in state 0 and we have the r_table with values [100, 1000] for the first row, action 1 will be selected as the index with the highest value is column 1.\n",
        "\n",
        "After the action has been selected and stored in a, this action is fed into the environment with env.step(a). This command returns the new state, the reward for this action, whether the game is “done” at this stage and the debugging information that we are not interested in. In the next line, the r_table cell corresponding to state s and action a is updated by adding the reward to whatever is already existing in the table cell.\n",
        "\n",
        "Finally the state s is updated to new_s – the new state of the agent.\n",
        "\n",
        "If we run this function, the r_table will look something like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfo26l60TnxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "eb240db9-cd8c-48f3-ebac-5bb359118431"
      },
      "source": [
        "naive_sum_reward_agent(env)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[     0., 560672.],\n",
              "       [ 27824.,      0.],\n",
              "       [     0.,  90460.],\n",
              "       [     0.,  18164.],\n",
              "       [ 91168.,      0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh7ZmQtuUits",
        "colab_type": "text"
      },
      "source": [
        "Examining the results above, you can observe that the most common state for the agent to be in is the first state, seeing as any action 1 will bring the agent back to this point. The least occupied state is state 4, as it is difficult for the agent to progress from state 0 to 4 without the action being “flipped” and the agent being sent back to state 0. You can get different results if you run the function multiple times, and this is because of the stochastic nature of both the environment and the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LZ_9r44VvUX",
        "colab_type": "text"
      },
      "source": [
        "Clearly – something is wrong with this table. One would expect that in state 4, the most rewarding action for the agent would be to choose action 0, which would reward the agent with 10 points, instead of the usual 2 points for an action of 1. Not only that, but it has chosen action 0 for all states – this goes against intuition – surely it would be best to sometimes shoot for state 4 by choosing multiple action 0’s in a row, and that way reap the reward of multiple possible 10 scores.\n",
        "\n",
        "In fact, there are a number of issues with this way of doing reinforcement learning:\n",
        "\n",
        "First, once there is a reward stored in one of the columns, the agent will always choose that action from that point on. This will lead to the table being “locked in” with respect to actions after just a few steps in the game.\n",
        "Second, because no reward is obtained for most of the states when action 0 is picked, this model for training the agent has no way to encourage acting on delayed reward signal when it is appropriate for it to do so.\n",
        "Let’s see how these problems could be fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuRUqkfnYxP6",
        "colab_type": "text"
      },
      "source": [
        "## Delayed reward reinforcement learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_04HBXV4jP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let’s say we are in state 3 – in the previous case, when the agent chose action 0 to get to state 3, the reward was zero and therefore r_table[3, 0] = 0. Obviously the agent would not see this as an attractive step compared to the alternative for this state i.e. r_table[3, 1] >= 2. But what if we assigned to this state the reward the agent would receive if it chose action 0 in state 4? It would look like this: r_table[3, 0] = r + 10 = 10 – a much more attractive alternative!\n",
        "\n",
        "This idea of propagating possible reward from the best possible actions in future states is a core component of what is called Q learning. In Q learning, the Q value for each action in each state is updated when the relevant information is made available. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhD0ryPCWSKJ",
        "colab_type": "text"
      },
      "source": [
        "$Q(s, a) = Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a’} Q(s’, a’) – Q(s, a))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61-a9QNYWf1w",
        "colab_type": "text"
      },
      "source": [
        "First, as you can observe, this is an updating rule – the existing Q value is added to, not replaced. Ignoring the alpha for the moment, we can concentrate on what’s inside the brackets. The first term, r, is the reward that was obtained when action a was taken in state s. Next, we have an expression which is a bit more complicated. Ignore the gamma for the moment and focus on $\\max\\limits_{a’} Q(s’, a’)$.\n",
        "\n",
        "What this means is that we look at the next state s’ after action a and return the maximum possible Q value in the next state. In other words, return the maximum Q value for the best possible action in the next state. In this way, the agent is looking forward to determine the best possible future rewards before making the next step a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ssyt75wW42Y",
        "colab_type": "text"
      },
      "source": [
        "The gamma value is called the discounting factor – this decreases the impact of future rewards on the immediate decision making in state s. This is important, as this represents a limited patience in the agent, so gamma will always be less than 1. The – Q(s, a) term acts to restrict the growth of the Q value as the training of the agent progresses through many iterations. Finally, this whole sum is multiplied by a learning rate alpha which restricts the updating to ensure it doesn’t “race” to a solution – this is important for optimal convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb9DHqO1XGy_",
        "colab_type": "text"
      },
      "source": [
        "Note that while the learning rule only examines the best action in the following state, in reality, discounted rewards still cascade down from future states. For instance, if we think of the cascading rewards from all the 0 actions (i.e. moving forward along the chain) and start at state 3, the Q reward will be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZkEPENEXHsY",
        "colab_type": "text"
      },
      "source": [
        "$r + \\gamma \\max_a Q(s’, a’) = 0 + 0.95 * 10 = 9.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3c1DihSXPly",
        "colab_type": "text"
      },
      "source": [
        "If we work back from state 3 to state 2 it will be 0 + 0.95 * 9.5 = 9.025. Likewise, the cascaded, discounted reward from to state 1 will be 0 + 0.95 * 9.025 = 8.57, and so on. Therefore, while the immediate updating calculation only looks at the maximum Q value for the next state, “upstream” rewards that have previously been discovered by the agent still cascade down into the present state and action decision. This is a simplification, due to the learning rate and random events in the environment, but represents the general idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4WEB7dEXlet",
        "colab_type": "text"
      },
      "source": [
        "The following is a possible implementation of the QLearning approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG7xjBZZW-Iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q_learning_with_table(env, num_episodes=500):\n",
        "    q_table = np.zeros((5, 2))\n",
        "    y = 0.95 # This is the gamma: Agent Patience factor\n",
        "    lr = 0.8 # This is the learning rate Alpha for convergence\n",
        "    for i in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.sum(q_table[s,:]) == 0:\n",
        "                # make a random selection of actions\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # select the action with largest q value in state s\n",
        "                a = np.argmax(q_table[s, :])\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            # The following line executes the Q learning rule that was presented previously. \n",
        "            # The np.max(q_table[new_s, :]) is an easy way of selecting the maximum value in the q_table for the row new_s\n",
        "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
        "            s = new_s\n",
        "    return q_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDkdKk3YOZW",
        "colab_type": "text"
      },
      "source": [
        "Let's execute the procedure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI0xMtvAXos5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "12268239-a493-48d7-aa3a-148c0d193626"
      },
      "source": [
        "q_learning_with_table(env)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21.27191013,  0.        ],\n",
              "       [ 0.        , 22.65391096],\n",
              "       [22.57979851,  0.        ],\n",
              "       [ 0.        , 27.14367692],\n",
              "       [28.01788348,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T5PakpBYXSC",
        "colab_type": "text"
      },
      "source": [
        "Again, we would expect at least the state 4 – action 0 combination to have the highest Q score, but it doesn’t.  We might also expect the reward from this action in this state to have cascaded down through the states 0 to 3. **Something has clearly gone wrong** – and the answer is that there isn’t enough exploration going on within the agent training method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvsB7DC9YkDD",
        "colab_type": "text"
      },
      "source": [
        "# Q learning with epsilon-greedy action selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLB7Rt65Yt1a",
        "colab_type": "text"
      },
      "source": [
        "If we think about the previous iteration of the agent training model using Q learning, the action selection policy is based solely on the maximum Q value in any given state. It is conceivable that, given the random nature of the environment, that the agent initially makes “bad” decisions. The Q values arising from these decisions may easily be “locked in” – and from that time forward, bad decisions may continue to be made by the agent because it can only ever select the maximum Q value in any given state, even if these values are not necessarily optimal. This action selection policy is called a greedy policy.\n",
        "\n",
        "So we need a way for the agent to eventually always choose the “best” set of actions in the environment, yet at the same time allowing the agent to not get “locked in” and giving it some space to explore alternatives. What is required is the epsilon-greedy policy.\n",
        "\n",
        "The epsilon-greedy policy in reinforcement learning is basically the same as the greedy policy, **except that there is a value  (which may be set to decay over time) where, if a random number is selected which is less than this value, an action is chosen completely at random**. This step allows some random exploration of the value of various actions in various states, and can be scaled back over time to allow the algorithm to concentrate more on exploiting the best strategies that it has found. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AJT1C-0ZTNB",
        "colab_type": "text"
      },
      "source": [
        "The following is a possible implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TaSUWquYJw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
        "    q_table = np.zeros((5, 2))\n",
        "    y = 0.95 # This is the gamma: Agent Patience factor\n",
        "    eps = 0.5 # Random threshold to trigger the Agent for random exploration \n",
        "    lr = 0.8 # This is the learning rate Alpha for convergence\n",
        "    decay_factor = 0.999 # exponentially decays eps with each episode eps *= decay_factor\n",
        "    for i in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        eps *= decay_factor\n",
        "        done = False\n",
        "        while not done:\n",
        "            # select the action with highest cummulative reward\n",
        "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
        "                # Triggers a random exploration\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # Proceed with greedy Q-learning\n",
        "                a = np.argmax(q_table[s, :])\n",
        "            # pdb.set_trace()\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
        "            s = new_s\n",
        "    return q_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdsZ2baBZYGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3749a7bb-100b-4d45-963a-0c09bf8198ce"
      },
      "source": [
        "eps_greedy_q_learning_with_table(env)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[62.39645877, 61.04611656],\n",
              "       [65.92907935, 62.37352545],\n",
              "       [73.88150447, 63.87966975],\n",
              "       [66.05076616, 43.55486257],\n",
              "       [91.35814383, 52.96667128]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFI00CiaPZZ",
        "colab_type": "text"
      },
      "source": [
        "**Finally we have a table which favors action 0 in state 4** – in other words what we would expect to happen given the reward of 10 that is up for grabs via that action in that state. Notice also that, as opposed to the previous tables from the other methods, that there are no actions with a 0 Q value – this is because the full action space has been explored via the randomness introduced by the epsilon-greedy policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqCNUefibEDT",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_nAHJVvbKuU",
        "colab_type": "text"
      },
      "source": [
        "So far, we have been dealing with explicit tables to hold information about the best actions and which actions to choose in any given state. However, while this is perfectly reasonable for a small environment like NChain, the table gets far too large and unwieldy for more complicated environments which have a huge number of states and potential actions.\n",
        "\n",
        "This is where neural networks can be used in reinforcement learning. Instead of having explicit tables, we can train a neural network to predict Q values for each action in a given state. This will be demonstrated using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpypKsMcbW8O",
        "colab_type": "text"
      },
      "source": [
        "To develop a neural network which can perform Q learning, the input needs to be the current state (plus potentially some other information about the environment) and it needs to output the relevant Q values for each action in that state. The Q values which are output should approach, as training progresses, the values produced in the Q learning updating rule. Therefore, the loss or cost function for the neural network should be:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVAikEm_bYCV",
        "colab_type": "text"
      },
      "source": [
        "$\\text{loss} = (\\underbrace{r + \\gamma \\max_{a’} Q'(s’, a’)}_{\\text{target}} – \\underbrace{Q(s, a)}_{\\text{prediction}})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BcuqsYwbyUo",
        "colab_type": "text"
      },
      "source": [
        "The input to the network is the one-hot encoded state vector. For instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]. In this case, a hidden layer of 10 nodes with sigmoid activation will be used. The output layer is a linear activated set of two nodes, corresponding to the two Q values assigned to each state to represent the two possible actions. Linear activation means that the output depends only on the linear summation of the inputs and the weights, with no additional function applied to that summation.\n",
        "\n",
        "The following is the neural network model to be used:\n",
        "\n",
        "\n",
        "![alt text](https://i0.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/03/Reinforcement-learning-Keras.png?zoom=3.5&resize=340%2C335&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWxvmk_zcVzo",
        "colab_type": "text"
      },
      "source": [
        "We create this NN architecture in Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAB54o2nZ_8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "02ae9eb5-b7b6-4a92-aab7-fc6db04b26b1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(batch_input_shape=(1, 5)))\n",
        "model.add(keras.layers.Dense(10, activation='sigmoid'))\n",
        "model.add(keras.layers.Dense(2, activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acx-yfw0dLv_",
        "colab_type": "text"
      },
      "source": [
        "First, the model is created using the Keras Sequential API. Then an input layer is added which takes inputs corresponding to the one-hot encoded state vectors. Then the sigmoid activated hidden layer with 10 nodes is added, followed by the linear activated output layer which will yield the Q values for each action. Finally the model is compiled using a mean-squared error loss function (to correspond with the loss function defined previously) with the Adam optimizer being used in its default Keras state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZbsjS2JdSsm",
        "colab_type": "text"
      },
      "source": [
        "We now integrate this model in the epsilon-greedy Q-learning approach described above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo_3W0ZDcfXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eps_greedy_q_learning_with_neural_network(env, num_episodes=500):\n",
        "  # now execute the q learning\n",
        "  y = 0.95\n",
        "  eps = 0.5\n",
        "  decay_factor = 0.999\n",
        "  r_avg_list = []\n",
        "  for i in range(num_episodes):\n",
        "      s = env.reset()\n",
        "      eps *= decay_factor\n",
        "      if i % 100 == 0:\n",
        "          print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
        "      done = False\n",
        "      r_sum = 0\n",
        "      while not done:\n",
        "          if np.random.random() < eps:\n",
        "              \"\"\"\n",
        "              Usual epsilon-greedy random exploration\n",
        "              \"\"\"\n",
        "              a = np.random.randint(0, 2)\n",
        "          else:\n",
        "              \"\"\"\n",
        "              The second condition uses the Keras model to produce the two Q values – one for each possible state. \n",
        "              It does this by calling the model.predict() function. Here the numpy identity function is used, with vector slicing, \n",
        "              to produce the one-hot encoding of the current state s. The standard numpy argmax function is used \n",
        "              to select the action with the highest Q value returned from the Keras model prediction.\n",
        "              \"\"\"\n",
        "              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
        "          new_s, r, done, _ = env.step(a)\n",
        "          # Model Updating\n",
        "          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
        "          target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
        "          target_vec[a] = target\n",
        "          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
        "          s = new_s\n",
        "          r_sum += r\n",
        "      r_avg_list.append(r_sum / 1000)\n",
        "  return r_avg_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSLpJMjreVEz",
        "colab_type": "text"
      },
      "source": [
        "Model updating is the main difference with the previous approach.\n",
        "\n",
        "The first line sets the target as the Q learning updating rule that has been previously presented. It is the reward r plus the discounted maximum of the predicted Q values for the new state, new_s. This is the value that we want the Keras model to learn to predict for state s and action a i.e. Q(s,a). However, our Keras model has an output for each of the two actions – we don’t want to alter the value for the other action, only the action a which has been chosen. So on the next line, target_vec is created which extracts both predicted Q values for state s. On the following line, only the Q value corresponding to the action a is changed to target – the other action’s Q value is left untouched.\n",
        "\n",
        "The final line is where the Keras model is updated in a single training step. The first argument is the current state – i.e. the one-hot encoded input to the model. The second is our target vector which is reshaped to make it have the required dimensions of (1, 2). The third argument tells the fit function that we only want to train for a single iteration and finally the verbose flag simply tells Keras not to print out the training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BehvmXCYdvkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bfc8a388-2d83-4a69-b819-5be946ee9e4b"
      },
      "source": [
        "r_avg_list = eps_greedy_q_learning_with_neural_network(env, 2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 1 of 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtAZFP8-fPLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "02c4f439-5af1-4605-a103-e1cdf90d92e1"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "x = np.linspace(0, 10, 500)\n",
        "ax.plot(x, r_avg_list);"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-62a9a949d63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_avg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'r_avg_list' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD5CAYAAADY+KXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQcAQ/4PmJPa2JPeZ7oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD5CAYAAADY+KXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQcAQ/4PmJPa2JPeZ7oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW6QTFoUf4Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}