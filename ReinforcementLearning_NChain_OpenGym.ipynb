{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearning-NChain-OpenGym.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thimotyb/real-world-machine-learning/blob/python3/ReinforcementLearning_NChain_OpenGym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwLWlFPuPLoB",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning\n",
        "## Open-AI Gym n-chain example with neural networks\n",
        "[link text](https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddtIhVpIPT3c",
        "colab_type": "text"
      },
      "source": [
        "The NChain example on Open AI Gym is a simple 5 state environment. There are two\n",
        "possible actions in each state, move forward (action 0) and move backwards (action 1).\n",
        "When action 1 is taken, i.e. move backwards, there is an immediate reward of 2 given to\n",
        "the agent – and the agent is returned to state 0 (back to the beginning of the chain).\n",
        "However, when a move forward action is taken (action 0), there is no immediate reward\n",
        "until state 4. When the agent moves forward while in state 4, a reward of 10 is received by\n",
        "the agent. The agent stays in state 4 at this point also, so the reward can be repeated.\n",
        "There is also a random chance that the agent’s action is “flipped” by the environment (i.e.\n",
        "an action 0 is flipped to an action 1 and vice versa). The diagram below demonstrates this\n",
        "environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWmjxGnFPaIO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzlo_aGpQBUq",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning – the basics\n",
        "Reinforcement learning can be considered the third genre of the machine learning triad – unsupervised learning, supervised learning and reinforcement learning. In supervised learning, we supply the machine learning system with curated (x, y) training pairs, where the intention is for the network to learn to map x to y. In reinforcement learning, we create an agent which performs actions in an environment and the agent receives various rewards depending on what state it is in when it performs the action. In other words, an agent explores a kind of game, and it is trained by trying to maximize rewards in this game. This cycle is illustrated in the figure below:\n",
        "\n",
        "![alt text](https://i2.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png?w=381&ssl=1)\n",
        "\n",
        "As can be observed above, the agent performs some action in the environment. An interpreter views this action in the environment, and feeds back an updated state that the agent now resides in, and also the reward for taking this action. The environment is not known by the agent beforehand, but rather it is discovered by the agent taking incremental steps in time. So, for instance, at time t the agent, in state ,  may take action a. This results in a new state  and a reward r. This reward can be a positive real number, zero, or a negative real number. It is the goal of the agent to learn which state dependent action to take which maximizes its rewards. The way which the agent optimally learns is the subject of reinforcement learning theory and methodologies.\n",
        "\n",
        "To more meaningfully examine the theory and possible approaches behind reinforcement learning, it is useful to have a simple example in which to work through. This simple example will come from an environment available on Open AI Gym called NChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZcAMmvQpNk",
        "colab_type": "text"
      },
      "source": [
        "## Playing with Gym\n",
        "\n",
        "The step() command returns 4 variables in a tuple, these are (in order):\n",
        "\n",
        "The new state after the action\n",
        "The reward due to the action\n",
        "Whether the game is “done” or not – the NChain game is done after 1,000 steps\n",
        "Debugging information – not relevant in this example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuscHiZjQteD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eabb2962-e9a0-476e-8894-eda6dd6f36ff"
      },
      "source": [
        "# Prepare the Gym\n",
        "import gym\n",
        "env = gym.make('NChain-v0')\n",
        "env.reset()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3T3lFvXQ1m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7805c3f3-b44c-4742-9e12-1f8cd5ea08c6"
      },
      "source": [
        "# Simulate a few steps of the game\n",
        "print(env.step(1))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0))\n",
        "print(env.step(0)) # Reaches the max reward in the game\n",
        "print(env.step(0))\n",
        "print(env.step(0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 2, False, {})\n",
            "(1, 0, False, {})\n",
            "(2, 0, False, {})\n",
            "(3, 0, False, {})\n",
            "(4, 0, False, {})\n",
            "(0, 2, False, {})\n",
            "(1, 0, False, {})\n",
            "(2, 0, False, {})\n",
            "(3, 0, False, {})\n",
            "(4, 0, False, {})\n",
            "(4, 10, False, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDdaZttRyKF",
        "colab_type": "text"
      },
      "source": [
        "As can be observed, starting in state 0 and taking step(1) action, the agent stays in state 0 and gets 2 for its reward. Next, I sent a series of action 0 commands. After every action 0 command, we would expect the progression of the agent along the chain, with the state increasing in increments (i.e. 0 -> 1 -> 2 etc.). However, you’ll observe after the first step(0) command, that the agent stays in state 0 and gets a 2 reward. This is because of the random tendency of the environment to “flip” the action occasionally, so the agent actually performed a 1 action. This is just unlucky.\n",
        "\n",
        "Nevertheless, I persevere and it can be observed that the state increments as expected, but there is no immediate reward for doing so for the agent until it reaches state 4. When in state 4, an action of 0 will keep the agent in step 4 and give the agent a 10 reward. Not only that, the environment allows this to be done repeatedly, as long as it doesn’t produce an unlucky “flip”, which would send the agent back to state 0 – the beginning of the chain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd9ZTgP2RJkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcvF-cyTKnX",
        "colab_type": "text"
      },
      "source": [
        "# A first naive heuristic for reinforcement learning\n",
        "\n",
        "In order to train the agent effectively, we need to find a good policy pi which maps states to actions in an optimal way to maximize reward. There are various ways of going about finding a good or optimal policy, but first, let’s consider a naive approach.\n",
        "\n",
        "Let’s conceptualize a table, and call it a reward table, which looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAqznm8SNyl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "$\\begin{bmatrix}r_{s_0,a_0} & r_{s_0,a_1}\\\\ r_{s_1,a_0} & r_{s_1,a_1} \\\\\n",
        "r_{s_2,a_0} & r_{s_2,a_1} \\\\\n",
        "r_{s_3,a_0} & r_{s_3,a_1} \\\\\n",
        "r_{s_4,a_0} & r_{s_4,a_1} \\end{bmatrix}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VmbEHQATcQl",
        "colab_type": "text"
      },
      "source": [
        "Each of the rows corresponds to the 5 available states in the NChain environment, and each column corresponds to the 2 available actions in each state – forward and backward, 0 and 1. The value in each of these table cells corresponds to some measure of reward that the agent has “learnt” occurs when they are in that state and perform that action. So, the value  would be, say, the sum of the rewards that the agent has received when in the past they have been in state 0 and taken action 0. This table would then let the agent choose between actions based on the summated (or average, median etc. – take your pick) amount of reward the agent has received in the past when taking actions 0 or 1.\n",
        "\n",
        "This might be a good policy – choose the action resulting in the greatest previous summated reward. Let’s give it a try, the code looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ywx61KSLJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_sum_reward_agent(env, num_episodes=500):\n",
        "    # this is the table that will hold our summated rewards for\n",
        "    # each action in each state\n",
        "    r_table = np.zeros((5, 2))\n",
        "    for g in range(num_episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.sum(r_table[s, :]) == 0:\n",
        "                # make a random selection of actions\n",
        "                a = np.random.randint(0, 2)\n",
        "            else:\n",
        "                # select the action with highest cummulative reward\n",
        "                a = np.argmax(r_table[s, :])\n",
        "            new_s, r, done, _ = env.step(a)\n",
        "            r_table[s, a] += r\n",
        "            s = new_s\n",
        "    return r_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfo26l60TnxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}